{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from scipy.sparse import coo_matrix\n",
    "import re, json, string, datetime, random, itertools\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import os\n",
    "\n",
    "%load_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/diya.li/.conda/envs/jupyterlab_2.0.1/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from corextopic import corextopic as ct\n",
    "import scipy.sparse as ss\n",
    "from corextopic import (\n",
    "    vis_topic as vt,\n",
    ")  # jupyter notebooks will complain matplotlib is being loaded twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "# got issues from https://stackoverflow.com/questions/26283715/how-to-find-the-most-similar-word-in-a-list-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopwords():\n",
    "    # read long stop words\n",
    "    punctuation = list(string.punctuation)\n",
    "    punctuation.remove(\"-\")\n",
    "    punctuation.remove(\"_\")\n",
    "    with open(\"long_stop_words.json\", \"r\") as f:\n",
    "        long_stop_list = json.load(f)\n",
    "    # get full stop words for this case\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    stoplist = long_stop_list + punctuation\n",
    "    stopwords.extend(stoplist)\n",
    "    stopwords.extend([\"al\", \"mon\", \"vis\"])\n",
    "    len(stopwords)\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def read_seed_list():\n",
    "    # read depression lexicon\n",
    "    with open(\"depression_lexicon-phq-9-new.json\") as f:\n",
    "        seed_terms = json.load(f)\n",
    "    all_seeds_raw = [\n",
    "        seed.replace(\"_\", \" \")\n",
    "        for seed in list(\n",
    "            itertools.chain.from_iterable(\n",
    "                [seed_terms[signal] for signal in seed_terms.keys()]\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    seed_lists = [\n",
    "        [item.replace(\"_\", \" \").lower() for item in seed_terms[k]]\n",
    "        for k in seed_terms.keys()\n",
    "    ]\n",
    "    return seed_lists\n",
    "\n",
    "\n",
    "def load_pretrained_glove():\n",
    "    scratch_path = os.environ[\"SCRATCH\"]\n",
    "    filename = \"glove.6B.50d.txt\"\n",
    "    glove_path = os.path.join(scratch_path, \"tmp/glove6B\")\n",
    "\n",
    "    embeddings_dict = {}\n",
    "    with open(os.path.join(glove_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            token = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[token] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "class BatchProcessSemiLDA:\n",
    "    def __init__(self):\n",
    "\n",
    "        # path config\n",
    "        self.scratch_path = os.environ[\"SCRATCH\"]\n",
    "        self.input_folder = \"covid-map/twitter-dataset-processed-stress\"\n",
    "        self.input_path = os.path.join(self.scratch_path, self.input_folder)\n",
    "\n",
    "        self.input_df_path = [\n",
    "            os.path.join(self.input_path, x)\n",
    "            for x in os.listdir(self.input_path)\n",
    "            if x.endswith(\".csv\")\n",
    "        ]\n",
    "\n",
    "        # csv path list splited by month\n",
    "        # self.tweets_filepath_set = self._read_dirs(self.input_path)\n",
    "        # sample for testing\n",
    "        # self.sample_json_path = self.tweets_filepath_set[\"2020-01\"][0]\n",
    "\n",
    "        #         self.output_folder = \"twitter-action/depression/2D-windows-stress-topic\"\n",
    "        self.output_folder = \"covid-map/twitter-dataset-processed-topic\"\n",
    "        self.output_folder_path = os.path.join(self.scratch_path, self.output_folder)\n",
    "\n",
    "        self.stress_rate_threshold = 0\n",
    "\n",
    "        # corpus\n",
    "        self.stopwords = read_stopwords()\n",
    "        self.seed_lists = read_seed_list()\n",
    "        # df_sample temp\n",
    "        self.df_sample = pd.DataFrame()\n",
    "        self.anchors = None\n",
    "        # used to fit word vector\n",
    "        self.embeddings_dict = load_pretrained_glove()\n",
    "        # topic columns\n",
    "        self.topic_columns = [\"topic_\" + str(x) for x in range(9)]\n",
    "\n",
    "    def _read_dirs(self, input_path):\n",
    "        tweets_file_set = {}\n",
    "        for month_folder in os.listdir(input_path):\n",
    "            if month_folder.startswith(\"2020\") and not month_folder.endswith(\".zip\"):\n",
    "                tweets_file_set[month_folder] = []\n",
    "                month_folder_path = os.path.join(input_path, month_folder)\n",
    "                # print(month_folder_path)\n",
    "                for tweets_file in os.listdir(month_folder_path):\n",
    "                    if tweets_file.endswith(\"csv\"):\n",
    "                        tweets_file_path = os.path.join(month_folder_path, tweets_file)\n",
    "                        tweets_file_set[month_folder].append(tweets_file_path)\n",
    "\n",
    "        print(\"filepath:\", tweets_file_set.keys())\n",
    "        # print(\"all file count\", sum([len(tweets_file_set[x]) for x in tweets_file_set]))\n",
    "        return tweets_file_set\n",
    "\n",
    "    def _write_df_windows(self, df_sample, opp):\n",
    "        df_sample.to_csv(opp, index=False)\n",
    "        print(\"writing df done.\")\n",
    "\n",
    "    def get_vectorizer_param(self, text_list):\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_df=0.3,\n",
    "            min_df=10,\n",
    "            max_features=None,\n",
    "            ngram_range=(1, 4),\n",
    "            norm=None,\n",
    "            binary=True,\n",
    "            use_idf=False,\n",
    "            sublinear_tf=False,\n",
    "            stop_words=self.stopwords,\n",
    "        )\n",
    "\n",
    "        vectorizer = vectorizer.fit(text_list)\n",
    "        tfidf = vectorizer.transform(text_list)\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "        print(\"vocab num\", len(vocab))\n",
    "        return tfidf, vocab\n",
    "\n",
    "    def find_closest_embeddings(self, embedding, cutoff=25):\n",
    "        return sorted(\n",
    "            self.embeddings_dict.keys(),\n",
    "            key=lambda token: spatial.distance.euclidean(\n",
    "                self, embeddings_dict[token], embedding\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def depression_lexicon_pattern(self, signal):\n",
    "        \"\"\"deprecated function\"\"\"\n",
    "        typical_signal_words = [\n",
    "            [word for word in topic if (\" \" not in word)] for topic in self.seed_lists\n",
    "        ]\n",
    "\n",
    "        # seleect the correct target_word by index\n",
    "        target_word_list = typical_signal_words[signal]\n",
    "\n",
    "        target_embedding = self.embeddings_dict[target_word]\n",
    "        for index, res in enumerate(typical_signal_words):\n",
    "            if index != signal:\n",
    "                # caculate all euclidean distance\n",
    "                target_embedding -= self.embeddings_dict[typical_signal_words[res]]\n",
    "        return target_embedding\n",
    "\n",
    "    def get_close_matches_anchor(self, seed_lists, vocab, anchor_index):\n",
    "        result = []\n",
    "        seed_index = 0\n",
    "        while len(result) <= 0:\n",
    "            seed_word = seed_lists[anchor_index][seed_index]\n",
    "            # print(\"seeding\", seed_word)\n",
    "\n",
    "            result = list(get_close_matches(seed_word, vocab))\n",
    "\n",
    "            seed_index += 1\n",
    "        # print(\"finally got one anchor\", result)\n",
    "        return result\n",
    "\n",
    "    def get_anchors(self, seed_lists, vocab):\n",
    "        anchor_list = []\n",
    "\n",
    "        anchor_list = [[a for a in topic if a in vocab] for topic in seed_lists]\n",
    "        anchors_len = [len(x) for x in anchor_list]\n",
    "        for index, l in enumerate(anchors_len):\n",
    "            if l == 0:\n",
    "                print(\"Got an zero anchor, finding similarity anchors\")\n",
    "                # anyway, we have to find a word to match the voab\n",
    "                anchor_list[index] = self.get_close_matches_anchor(\n",
    "                    seed_lists, vocab, index\n",
    "                )\n",
    "        # look fine, return\n",
    "        return anchor_list\n",
    "\n",
    "    def train_model(self, X, words, anchors, anchor_strength=3):\n",
    "        print(\"trainning model\", end=\"\\r\")\n",
    "        # Train the first layer\n",
    "        model = ct.Corex(n_hidden=20, seed=8)\n",
    "        model = model.fit(\n",
    "            X,\n",
    "            words=words,\n",
    "            anchors=anchors,  # Pass the anchors in here\n",
    "            anchor_strength=anchor_strength,  # Tell the model how much it should rely on the anchors\n",
    "        )\n",
    "        return model\n",
    "\n",
    "        # TODO: Train successive layers\n",
    "        tm_layer2 = ct.Corex(n_hidden=10, seed=16)\n",
    "        tm_layer2.fit(model.labels)\n",
    "\n",
    "        tm_layer3 = ct.Corex(n_hidden=9)\n",
    "        tm_layer3.fit(\n",
    "            tm_layer2.labels,\n",
    "            words=words,\n",
    "            anchors=anchors,  # Pass the anchors in here\n",
    "            anchor_strength=anchor_strength,  # Tell the model how much it should rely on the anchors\n",
    "            verbose=1,\n",
    "            max_iter=300,\n",
    "        )\n",
    "        print(\"finished\")\n",
    "        return tm_layer3\n",
    "\n",
    "    def _write_log(self, log_text_list, r_opp):\n",
    "        with open(r_opp, \"w\") as f:\n",
    "            for log in log_text_list:\n",
    "                f.write(\"%s\\n\" % log)\n",
    "\n",
    "    def print_model_topic_result(self, model, anchor_num):\n",
    "        result_list = []\n",
    "        for n in range(anchor_num):\n",
    "            topic_words, _ = zip(*model.get_topics(topic=n))\n",
    "            result = \"{}: \".format(n) + \",\".join(topic_words)\n",
    "            result_list.append(result)\n",
    "            print(result)\n",
    "        return result_list\n",
    "\n",
    "    def get_processed_df(self, model, X):\n",
    "        model_labels = model.transform(X)\n",
    "\n",
    "        # select columns https://thispointer.com/python-numpy-select-rows-columns-by-index-from-a-2d-ndarray-multi-dimension/\n",
    "        model_labels = model_labels[:, : len(self.topic_columns)]\n",
    "\n",
    "        # get topic distribution model\n",
    "        topic_df = pd.DataFrame(model_labels, columns=self.topic_columns).astype(\n",
    "            float\n",
    "        )  # save space\n",
    "\n",
    "        topic_df.index = self.df_sample.index\n",
    "        df = pd.concat([self.df_sample, topic_df], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_n_process_df(self, sample_df_path):\n",
    "        print(\"reading\", sample_df_path)\n",
    "        self.df_sample = pd.read_csv(sample_df_path, lineterminator=\"\\n\")\n",
    "        # code below are processed\n",
    "        # self.df_sample = self.df_sample[self.df_sample[\"lang\"] == \"en\"]\n",
    "        # self.df_sample = self.df_sample.drop_duplicates(subset=\"cleaned_text\")\n",
    "        # df_sample = df_sample[df_sample[\"cleaned_text\"].notna()]\n",
    "        # df_sample = df_sample[df_sample[\"place_type\"].notna()]\n",
    "\n",
    "        self.df_sample = self.df_sample[\n",
    "            self.df_sample[\"stress_rate\"] >= self.stress_rate_threshold\n",
    "        ]\n",
    "        print(self.df_sample.shape, \"starting...\")\n",
    "        X, vocab = self.get_vectorizer_param(self.df_sample[\"cleaned_text\"])\n",
    "        self.anchors = self.get_anchors(self.seed_lists, vocab)\n",
    "\n",
    "        if self.anchors:\n",
    "            # train model\n",
    "            model = self.train_model(X, vocab, self.anchors, anchor_strength=3)\n",
    "            result_text_list = self.print_model_topic_result(\n",
    "                model, len(self.anchors)\n",
    "            )  # print result\n",
    "\n",
    "            processed_df = self.get_processed_df(model, X)\n",
    "            return processed_df, result_text_list\n",
    "\n",
    "    def start_one(self, df_path):\n",
    "        # input df: cleaned text must be cleaned\n",
    "        df_file_names = df_path.split(\"/\")[-1]\n",
    "        opp = os.path.join(self.output_folder_path, df_file_names)\n",
    "        r_opp = os.path.join(\n",
    "            self.output_folder_path, df_file_names.replace(\"csv\", \"txt\")\n",
    "        )\n",
    "        if os.path.isfile(opp):\n",
    "            # skip exists file\n",
    "            return\n",
    "        processed_df, result_text_list = self.load_n_process_df(df_path)\n",
    "        print(\"Start writing to\", opp)\n",
    "        self._write_df_windows(processed_df, opp)\n",
    "        self._write_log(result_text_list, r_opp)\n",
    "        # return processed_df\n",
    "\n",
    "    def start_all(self, df_path_list):\n",
    "        for df_path in df_path_list:\n",
    "            self.start_one(df_path)\n",
    "            print(\"===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "bps = BatchProcessSemiLDA()\n",
    "df_path_list = bps.input_df_path\n",
    "bps.start_all(df_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_path_list = bps.tweets_filepath_set[\"2020-01\"]\n",
    "# bps.start_all(df_path_list)\n",
    "# df_path_list = bps.tweets_filepath_set[\"2020-02\"]\n",
    "# bps.start_all(df_path_list)\n",
    "# df_path_list = bps.tweets_filepath_set[\"2020-03\"]\n",
    "# bps.start_all(df_path_list)\n",
    "# df_path_list = bps.tweets_filepath_set[\"2020-04\"]\n",
    "# bps.start_all(df_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchProcessSemiLDA' object has no attribute 'df_path_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8ff12a60acad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_path_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchProcessSemiLDA' object has no attribute 'df_path_list'"
     ]
    }
   ],
   "source": [
    "bps.df_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# syns = wordnet.synsets(\"better off death\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# # Then, we're going to use the term \"program\" to find synsets like so:\n",
    "\n",
    "\n",
    "# # An example of a synset:\n",
    "# print(syns[0].name())\n",
    "\n",
    "# # Just the word:\n",
    "# print(syns[0].lemmas()[0].name())\n",
    "\n",
    "# # Definition of that first synset:\n",
    "# print(syns[0].definition())\n",
    "\n",
    "# # Examples of the word in use in sentences:\n",
    "# print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
