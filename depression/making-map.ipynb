{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from shapely.geometry import shape\n",
    "\n",
    "%load_ext lab_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeMap:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.base_path = os.environ[\"SCRATCH\"]\n",
    "        input_folder = \"covid-map/twitter-dataset-processed-stress\"\n",
    "        self.input_folder_path = os.path.join(self.base_path, input_folder)\n",
    "\n",
    "        # zip file smaple\n",
    "        zip_county_shp = \"twitter-action/depression/county-map/county.zip\"\n",
    "        self.zip_county_shp_path = (\n",
    "            \"zip://\" + os.path.join(self.base_path, zip_county_shp) + \"!data\"\n",
    "        )\n",
    "\n",
    "        # shape file sample\n",
    "        county_shp = \"twitter-action/depression/basemap/data/data_2.shp\"\n",
    "        self.county_shp_path = os.path.join(self.base_path, county_shp)\n",
    "\n",
    "        basemap = \"twitter-action/depression/basemap/basemap.geojson\"\n",
    "        self.basemap_path = os.path.join(self.base_path, basemap)\n",
    "\n",
    "        self.sample_input_path = \"/scratch/user/diya.li/twitter-action/depression/2D-windows-stress-topic/2020-02-22.csv\"\n",
    "\n",
    "        self.map_output_path = os.path.join(\n",
    "            self.base_path, \"twitter-action/depression/county-map\"\n",
    "        )\n",
    "\n",
    "        # get all csv path from input folder\n",
    "        self.csv_path_list = self._get_df_path_list()\n",
    "\n",
    "        self.stress_df_columns = [\n",
    "            \"FID\",\n",
    "            \"cnty_fips\",\n",
    "            \"state_name\",\n",
    "            \"state_fips\",\n",
    "            \"cnty_name\",\n",
    "            \"state_abbr\",\n",
    "            \"geometry\",\n",
    "            \"avg_stress\",\n",
    "            \"date\",\n",
    "        ]\n",
    "\n",
    "        self.basemap_shp = geopandas.read_file(self.basemap_path)\n",
    "\n",
    "    def test_plot(self, shp_obj):\n",
    "        \"\"\"deprecated function\"\"\"\n",
    "        world = geopandas.read_file(geopandas.datasets.get_path(\"naturalearth_lowres\"))\n",
    "        world = world[world[\"continent\"] == \"North America\"]\n",
    "        # base = world.plot(color=\"white\", edgecolor=\"black\")\n",
    "        shp_obj.plot()\n",
    "\n",
    "    def preprocess_county_shp(self, shp_obj):\n",
    "        \"\"\"deprecated function\"\"\"\n",
    "        shp_obj[\"cnty_fips\"] = shp_obj[\"cnty_fips\"].apply(lambda x: np.float64(x))\n",
    "\n",
    "        shp_obj = shp_obj.drop(\n",
    "            [\n",
    "                \"OBJECTID\",\n",
    "                \"Value\",\n",
    "                \"Shape_Leng\",\n",
    "                \"SmoValue\",\n",
    "                \"DrValue\",\n",
    "                \"InacValue\",\n",
    "                \"MedCValue\",\n",
    "                \"Income\",\n",
    "                \"Shape_Le_1\",\n",
    "                \"Shape_Area\",\n",
    "                \"Deathrate\",\n",
    "                \"Diabetes\",\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        # shp_obj = shp_obj[shp_obj[\"state_name\"] != \"Virgin Islands of the U.S.\"]\n",
    "        # shp_obj.to_crs(\"EPSG:3395\")\n",
    "        return shp_obj\n",
    "\n",
    "    def all_county_stress_rate(self, df, shp_obj):\n",
    "        \"\"\"\n",
    "        deprecated function\n",
    "        \"\"\"\n",
    "        # shp_obj[\"cnty_fips\"] = shp_obj[\"cnty_fips\"].apply(lambda x: str(np.float64(x)))\n",
    "\n",
    "        avg_stress_set = {}\n",
    "\n",
    "        for group in df.groupby(\"CountyId\"):\n",
    "            avg_stress = sum(group[1][\"stress_rate\"].values) / group[1].shape[0]\n",
    "            county_df_num = group[0]\n",
    "            # print(county_df[0], county_df_num, avg_stress)\n",
    "            # shp_obj.loc[\n",
    "            #    shp_obj[\"cnty_fips\"] == county_df_num, [\"avg_stress\"]\n",
    "            # ] = avg_stress\n",
    "            avg_stress_set[county_df_num] = avg_stress\n",
    "        # print(avg_stress_set)\n",
    "        shp_obj[\"avg_stress\"] = shp_obj[\"cnty_fips\"].apply(\n",
    "            lambda x: avg_stress_set.get(x)\n",
    "        )\n",
    "\n",
    "        return shp_obj\n",
    "\n",
    "    def write_shp(self, shp_obj, output_path, driver=\"ESRI Shapefile\"):\n",
    "        shp_obj.to_file(output_path, driver=driver)\n",
    "\n",
    "    def _get_df_path_list(self):\n",
    "        l = []\n",
    "        for df_path in os.listdir(self.input_folder_path):\n",
    "            if df_path.endswith(\"csv\"):\n",
    "                l.append(df_path)\n",
    "        l.sort()\n",
    "        return l\n",
    "\n",
    "    def read_one_csv(self, df_path):\n",
    "        df = pd.read_csv(df_path, lineterminator=\"\\n\")\n",
    "        return df\n",
    "\n",
    "    def zipdir(self, path, ziph):\n",
    "        # ziph is zipfile handle\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                ziph.write(os.path.join(root, file))\n",
    "\n",
    "    def zip_list_file(self, input_file_name):\n",
    "        # input_file_path e.g. 2020-01-28\n",
    "        zipf = zipfile.ZipFile(\n",
    "            self.map_output_path + \"/{}.zip\".format(input_file_name.replace(\"-\", \"\")),\n",
    "            \"w\",\n",
    "            zipfile.ZIP_DEFLATED,\n",
    "        )\n",
    "\n",
    "        input_file_path = os.path.join(self.map_output_path, input_file_name)\n",
    "\n",
    "        self.zipdir(input_file_path, zipf)\n",
    "        zipf.close()\n",
    "\n",
    "    def read_csv_folder_with_stress(self, input_folder_path, concated=False):\n",
    "        \"\"\"is function is used to group the stress string by date\"\"\"\n",
    "        nums = len(self.csv_path_list)\n",
    "        concated_df = pd.DataFrame()\n",
    "        with tqdm(total=nums) as pbar:\n",
    "            for df_path in self.csv_path_list:\n",
    "\n",
    "                df_path = os.path.join(input_folder_path, df_path)\n",
    "                df = self.read_one_csv(df_path)\n",
    "\n",
    "                # handle data\n",
    "                date_string = df_path.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "                date_obj = str(pd.to_datetime(date_string)) + \"+00:00\"\n",
    "                row_list = []\n",
    "\n",
    "                # check path\n",
    "                output_path = os.path.join(\n",
    "                    os.path.abspath(\"./county-map\"), date_string.replace(\"-\", \"\")\n",
    "                )\n",
    "\n",
    "                if os.path.isdir(output_path):\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                if os.path.isfile(os.path.join(output_path + \".geojson\")):\n",
    "                    pbar.update(1)\n",
    "                    print(\"skipping\")\n",
    "                    continue\n",
    "\n",
    "                for group in df.groupby(\"CountyId\"):\n",
    "                    countyid = group[0]\n",
    "                    row = self.basemap_shp[\n",
    "                        self.basemap_shp[\"cnty_fips\"] == countyid\n",
    "                    ].values.tolist()\n",
    "                    if len(row) == 0:\n",
    "                        continue\n",
    "                    row = row[0]\n",
    "\n",
    "                    avg_stress = sum(group[1][\"stress_rate\"].values) / group[1].shape[0]\n",
    "\n",
    "                    row.append(avg_stress)\n",
    "\n",
    "                    if concated:\n",
    "                        ## MARKER concated\n",
    "                        date_obj = pd.to_datetime(date_obj)\n",
    "                        row.append(date_obj)\n",
    "                    else:\n",
    "                        row.append(date_obj)\n",
    "\n",
    "                    row_list.append(row)\n",
    "\n",
    "                # print(row_list[0])\n",
    "                new_df = pd.DataFrame(row_list, columns=self.stress_df_columns)\n",
    "                if concated:\n",
    "                    ## MARKER concated\n",
    "                    # combine all the csv to one file\n",
    "                    concated_df = pd.concat([concated_df, new_df])\n",
    "                else:\n",
    "                    # write to geojson\n",
    "                    gdf = geopandas.GeoDataFrame(new_df, geometry=new_df.geometry)\n",
    "                    gdf_path = os.path.join(output_path + \".geojson\")\n",
    "                    print(gdf_path)\n",
    "                    # os.mkdir(output_path)\n",
    "                    MM.write_shp(gdf, gdf_path, driver=\"GeoJSON\")\n",
    "                    # self.zip_list_file(date_string)\n",
    "                pbar.update(1)\n",
    "\n",
    "            if concated:\n",
    "                ## MARKER concated\n",
    "                # write concated df to geojson\n",
    "                concated_gdf = geopandas.GeoDataFrame(\n",
    "                    concated_df, geometry=concated_df.geometry\n",
    "                )\n",
    "                concated_gdf_path = os.path.join(output_path + \".geojson\")\n",
    "                print(concated_gdf_path)\n",
    "                # os.mkdir(output_path)\n",
    "                MM.write_shp(concated_gdf, concated_gdf_path, driver=\"GeoJSON\")\n",
    "        # return concated_df\n",
    "\n",
    "    def read_csv_folder(self, csv_path_list):\n",
    "        \"\"\"\n",
    "        read all csv from a folder \n",
    "        \"\"\"\n",
    "        nums = len(csv_path_list)\n",
    "        concated_df = pd.DataFrame()\n",
    "        with tqdm(total=nums) as pbar:\n",
    "            for df_path in csv_path_list:\n",
    "                df_path = os.path.join(self.input_folder_path, df_path)\n",
    "                date_string = df_path.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "                df = self.read_one_csv(df_path)\n",
    "                df[\"date\"] = pd.to_datetime(date_string)\n",
    "                concated_df = pd.concat([concated_df, df])\n",
    "                pbar.update(1)\n",
    "        return concated_df\n",
    "\n",
    "\n",
    "class MakeTopicMap(MakeMap):\n",
    "    def __init__(self):\n",
    "        MakeMap.__init__(self)\n",
    "\n",
    "    def add_geometory_to_topic(self, df, shp_obj):\n",
    "        \"\"\"todo\"\"\"\n",
    "        nums = df.shape[0]\n",
    "        with tqdm(total=nums) as pbar:\n",
    "            for index in range(nums):\n",
    "                countyId = str(df.iloc[index][\"CountyId\"])\n",
    "\n",
    "    def write_topic(self, concated_df):\n",
    "        topic_names = [\"topic_{}\".format(x) for x in range(9)]\n",
    "        for topic in topic_names:\n",
    "            temp_df = concated_df[concated_df[topic] != 0.0]\n",
    "            #     temp_df.to_csv(\"topic-map/{}.csv\".format(topic), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [00:01<00:19,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [00:05<00:31,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [00:08<00:29,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [00:09<00:23,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [00:10<00:19,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6/16 [00:11<00:16,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [00:12<00:12,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:15<00:14,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "/scratch/user/diya.li/twitter-action/depression/county-map/20200301.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [00:29<00:38,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/diya.li/twitter-action/depression/county-map/20200306.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [00:38<00:40,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/diya.li/twitter-action/depression/county-map/20200311.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [00:48<00:37,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/diya.li/twitter-action/depression/county-map/20200316.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [00:57<00:32,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/diya.li/twitter-action/depression/county-map/20200321.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [01:00<00:19,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/diya.li/twitter-action/depression/county-map/20200331.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [01:07<00:13,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/diya.li/twitter-action/depression/county-map/20200405.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [01:15<00:07,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/user/diya.li/twitter-action/depression/county-map/20200410.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:20<00:00,  5.03s/it]\n"
     ]
    }
   ],
   "source": [
    "MM = MakeMap()\n",
    "MTM = MakeTopicMap()\n",
    "# preprocess base map\n",
    "# tmp_shp = MM.basemap_shp\n",
    "# tmp_shp = MM.preprocess_county_shp(tmp_shp)\n",
    "# MM.write_shp(tmp_shp, output_path=\"./basemap/basemap.geojson\", driver=\"GeoJSON\")\n",
    "##### done\n",
    "\n",
    "\n",
    "# tmp_df = MM.read_one_csv(MM.sample_input_path)\n",
    "# tmp_shp = MM.basemap_shp\n",
    "MM.read_csv_folder_with_stress(MM.input_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concated_df = MM.read_csv_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tmp_shp = MM.all_county_stress_rate(concated_df_unique, tmp_shp)\n",
    "# concated_df = MM.read_csv_folder_with_stress(shp_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MM.write_shp(tmp_shp, \"./temp_map/sum.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.GeoDataFrame(concated_df, geometry=concated_df.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-22 00:00:00+00:00'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf[\"date\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf[\"date\"] = gdf[\"date\"].apply(lambda x: str(x))\n",
    "# gdf[\"date\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>cnty_fips</th>\n",
       "      <th>state_name</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>cnty_name</th>\n",
       "      <th>state_abbr</th>\n",
       "      <th>Value</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>SmoValue</th>\n",
       "      <th>DrValue</th>\n",
       "      <th>InacValue</th>\n",
       "      <th>MedCValue</th>\n",
       "      <th>Income</th>\n",
       "      <th>Shape_Le_1</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>Deathrate</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>01001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>AL</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0</td>\n",
       "      <td>28.88</td>\n",
       "      <td>6.3</td>\n",
       "      <td>30.3</td>\n",
       "      <td>17780</td>\n",
       "      <td>54.5</td>\n",
       "      <td>229109.169793</td>\n",
       "      <td>2.188822e+09</td>\n",
       "      <td>396.1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>POLYGON ((-86.82067 32.34731, -86.81446 32.370...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>01003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>AL</td>\n",
       "      <td>31.3</td>\n",
       "      <td>0</td>\n",
       "      <td>28.96</td>\n",
       "      <td>9.3</td>\n",
       "      <td>22.6</td>\n",
       "      <td>16243</td>\n",
       "      <td>56.5</td>\n",
       "      <td>442509.254514</td>\n",
       "      <td>5.829047e+09</td>\n",
       "      <td>352.4</td>\n",
       "      <td>9.3</td>\n",
       "      <td>POLYGON ((-87.97309 31.16482, -87.93710 31.173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>01005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>AL</td>\n",
       "      <td>44.7</td>\n",
       "      <td>0</td>\n",
       "      <td>34.27</td>\n",
       "      <td>6.4</td>\n",
       "      <td>27.4</td>\n",
       "      <td>19547</td>\n",
       "      <td>32.9</td>\n",
       "      <td>266874.292769</td>\n",
       "      <td>3.245946e+09</td>\n",
       "      <td>422.7</td>\n",
       "      <td>16.2</td>\n",
       "      <td>POLYGON ((-85.74337 31.62624, -85.71720 31.679...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>01007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>AL</td>\n",
       "      <td>37.9</td>\n",
       "      <td>0</td>\n",
       "      <td>34.74</td>\n",
       "      <td>5.5</td>\n",
       "      <td>34.1</td>\n",
       "      <td>19462</td>\n",
       "      <td>43.1</td>\n",
       "      <td>223820.703443</td>\n",
       "      <td>2.322895e+09</td>\n",
       "      <td>437.3</td>\n",
       "      <td>13.7</td>\n",
       "      <td>POLYGON ((-87.41986 33.01177, -87.31532 33.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>01009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Blount</td>\n",
       "      <td>AL</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0</td>\n",
       "      <td>31.46</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.3</td>\n",
       "      <td>18771</td>\n",
       "      <td>47.2</td>\n",
       "      <td>249967.888132</td>\n",
       "      <td>2.503708e+09</td>\n",
       "      <td>434.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>POLYGON ((-86.96799 33.86045, -86.92667 33.872...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FID  OBJECTID cnty_fips state_name state_fips cnty_name state_abbr  Value  \\\n",
       "0    1         1     01001    Alabama         01   Autauga         AL   37.6   \n",
       "1    2         2     01003    Alabama         01   Baldwin         AL   31.3   \n",
       "2    3         3     01005    Alabama         01   Barbour         AL   44.7   \n",
       "3    4         4     01007    Alabama         01      Bibb         AL   37.9   \n",
       "4    5         5     01009    Alabama         01    Blount         AL   34.6   \n",
       "\n",
       "   Shape_Leng  SmoValue  DrValue  InacValue  MedCValue  Income     Shape_Le_1  \\\n",
       "0           0     28.88      6.3       30.3      17780    54.5  229109.169793   \n",
       "1           0     28.96      9.3       22.6      16243    56.5  442509.254514   \n",
       "2           0     34.27      6.4       27.4      19547    32.9  266874.292769   \n",
       "3           0     34.74      5.5       34.1      19462    43.1  223820.703443   \n",
       "4           0     31.46      5.0       27.3      18771    47.2  249967.888132   \n",
       "\n",
       "     Shape_Area  Deathrate  Diabetes  \\\n",
       "0  2.188822e+09      396.1      13.0   \n",
       "1  5.829047e+09      352.4       9.3   \n",
       "2  3.245946e+09      422.7      16.2   \n",
       "3  2.322895e+09      437.3      13.7   \n",
       "4  2.503708e+09      434.7      12.4   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-86.82067 32.34731, -86.81446 32.370...  \n",
       "1  POLYGON ((-87.97309 31.16482, -87.93710 31.173...  \n",
       "2  POLYGON ((-85.74337 31.62624, -85.71720 31.679...  \n",
       "3  POLYGON ((-87.41986 33.01177, -87.31532 33.012...  \n",
       "4  POLYGON ((-86.96799 33.86045, -86.92667 33.872...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_shp = geopandas.read_file(\"./basemap/basemap.geojson\")\n",
    "tmp_shp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>cnty_fips</th>\n",
       "      <th>state_name</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>cnty_name</th>\n",
       "      <th>state_abbr</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>AL</td>\n",
       "      <td>POLYGON ((-86.82067 32.34731, -86.81446 32.370...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>01003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>AL</td>\n",
       "      <td>POLYGON ((-87.97309 31.16482, -87.93710 31.173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>01005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>AL</td>\n",
       "      <td>POLYGON ((-85.74337 31.62624, -85.71720 31.679...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>01007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>AL</td>\n",
       "      <td>POLYGON ((-87.41986 33.01177, -87.31532 33.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>01009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01</td>\n",
       "      <td>Blount</td>\n",
       "      <td>AL</td>\n",
       "      <td>POLYGON ((-86.96799 33.86045, -86.92667 33.872...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FID cnty_fips state_name state_fips cnty_name state_abbr  \\\n",
       "0    1     01001    Alabama         01   Autauga         AL   \n",
       "1    2     01003    Alabama         01   Baldwin         AL   \n",
       "2    3     01005    Alabama         01   Barbour         AL   \n",
       "3    4     01007    Alabama         01      Bibb         AL   \n",
       "4    5     01009    Alabama         01    Blount         AL   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-86.82067 32.34731, -86.81446 32.370...  \n",
       "1  POLYGON ((-87.97309 31.16482, -87.93710 31.173...  \n",
       "2  POLYGON ((-85.74337 31.62624, -85.71720 31.679...  \n",
       "3  POLYGON ((-87.41986 33.01177, -87.31532 33.012...  \n",
       "4  POLYGON ((-86.96799 33.86045, -86.92667 33.872...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tmp_shp = MM.preprocess_county_shp(tmp_shp)\n",
    "tmp_shp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_rate_list = []\n",
    "for county_df in concated_df_unique.groupby(\"CountyId\"):\n",
    "\n",
    "    avg_stress = sum(county_df[1][\"stress_rate\"].values) / county_df[1].shape[0]\n",
    "    # print(county_df[0], county_df[1].shape, avg_stress)\n",
    "    stress_rate_list.append(avg_stress)\n",
    "    # stress_rate/num tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
