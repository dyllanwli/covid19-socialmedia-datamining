{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "# import daemon\n",
    "from config import getConfig\n",
    "\n",
    "def test(api):\n",
    "    # Test api\n",
    "    public_tweets = api.home_timeline()\n",
    "    print(len(public_tweets))\n",
    "\n",
    "\n",
    "def load_config_tk(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        consumer_key, consumer_secret, access_token, access_token_secret = [line.strip() for line in f.readlines()]\n",
    "        print(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "        return consumer_key, consumer_secret, access_token, access_token_secret\n",
    "\n",
    "def loadFilter(filename):\n",
    "    with open(filename, \"r\") as filterJson:\n",
    "        j = json.load(filterJson)\n",
    "        return j\n",
    "    \n",
    "def sperateBoundbox(degree):\n",
    "    minlong = boundbox[0]\n",
    "    minlat = boundbox[1]\n",
    "    maxlong = boundbox[2]\n",
    "    maxlat = boundbox[3]\n",
    "\n",
    "    degree = 1000\n",
    "\n",
    "    boxs = []\n",
    "    detal_lat = (maxlat - minlat) / degree\n",
    "    detal_long = (maxlong - minlong) / degree\n",
    "\n",
    "    l1 = minlong\n",
    "    l2 = minlat\n",
    "\n",
    "    for x in range(degree):\n",
    "        l1 = minlong\n",
    "        l2 += detal_lat\n",
    "        l4 = l2 + detal_lat\n",
    "        for y in range(degree):\n",
    "            l1 += detal_long\n",
    "            l3 = l1 + detal_long\n",
    "            box = [l1, l2, l3, l4]\n",
    "            boxs.append(box)\n",
    "    return boxs\n",
    "\n",
    "\n",
    "def create_csv(filename):\n",
    "    print(\"Start running on\", filename)\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        if getAllTweets:\n",
    "            writer.writerow(title)\n",
    "        else:\n",
    "            writer.writerow(jsonKeys)\n",
    "\n",
    "\n",
    "def write_csv(data):\n",
    "    filename = datetime.now().strftime(\"%Y%m%d-%H\") + \".csv\"\n",
    "    if os.path.isfile(filename):\n",
    "        pass\n",
    "    else:\n",
    "        create_csv(filename)\n",
    "    with open(filename, \"a\") as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(data)\n",
    "        \n",
    "\n",
    "with open('big_json_array.json', 'w') as out:\n",
    "    json.dump(IteratorAsList(some_very_big_iterator), out)\n",
    "\n",
    "def removeStopWord(text):\n",
    "    return ' '.join([word for word in text.split() if word not in tracks])\n",
    "        \n",
    "def cursorGenerator(api):\n",
    "    filename = datetime.now().strftime(\"%Y%m%d-%H-%M\") + \".json\"\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"{} existst, return\".format(filename))\n",
    "        return\n",
    "    with open(filename, 'w') as outfile:\n",
    "        for status in tweepy.Cursor(api.search, q=search_term, since = '2020-03-01').items():\n",
    "            print(status.text)\n",
    "            break\n",
    "            contents = []\n",
    "            # process status here\n",
    "            text = removeStopWord(status.text)\n",
    "            if len(text) > 1:               \n",
    "                # only essentail data \n",
    "                tweetJson = status._json\n",
    "                for k in jsonKeys:\n",
    "                    contents.append(tweetJson[k])\n",
    "            json.dump(IteratorAsList(some_very_big_iterator), out)\n",
    "\n",
    "class IteratorAsList(list):\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "    def __iter__(self):\n",
    "        return self.it\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "            \n",
    "########\n",
    "# setup config\n",
    "tokenPath = '../token.tk'\n",
    "consumer_key, consumer_secret, access_token, access_token_secret = load_config_tk(tokenPath)\n",
    "\n",
    "config = getConfig()\n",
    "title = config['title']\n",
    "jsonKeys = config['jsonKeys']\n",
    "tracks = config['track']\n",
    "# search_term = \"{} -filter:retweets\".format(\" OR \".join(tracks))\n",
    "search_term = \"{}\".format(\" OR \".join(tracks))\n",
    "\n",
    "#setup boundbox\n",
    "filters = loadFilter(\"./filter.json\")\n",
    "#setup geoconfig: if Ture, get every tweet; if False, only tweet with geotag will write down\n",
    "ignoreGeoTag = False\n",
    "# setup if all the twitter data is needed, not recommend cuz it may cause lots of unnecessary data\n",
    "getAllTweets = False\n",
    "########\n",
    "\n",
    "def main():\n",
    "    # setup api\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    # test\n",
    "    test(api)\n",
    "    \n",
    "    # with daemon.DaemonContext():\n",
    "    print(\"Start running on\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    cursorGenerator(api)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start running...\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
