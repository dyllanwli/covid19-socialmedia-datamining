{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from scipy.sparse import coo_matrix\n",
    "import re, json, string, datetime, random, itertools\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import os\n",
    "\n",
    "%load_ext lab_black\n",
    "%matplotlib inline\n",
    "# got an # This is an internal API and we don't need nested context for this. issues if you reloaded that\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "class GetStress:\n",
    "    def __init__(self):\n",
    "\n",
    "        # sys\n",
    "        os.environ[\"TFHUB_CACHE_DIR\"] = os.environ[\"SCRATCH\"] + \"/tmp/tfhub\"\n",
    "        ## path\n",
    "        self.scratch_path = os.environ[\"SCRATCH\"]\n",
    "        self.input_folder = \"covid-map/twitter-dataset-processed-2\"\n",
    "        self.input_path = os.path.join(self.scratch_path, self.input_folder)\n",
    "        self.output_folder = \"covid-map/twitter-dataset-processed-stress\"\n",
    "\n",
    "        # csv path list splited by month\n",
    "        # self.tweets_filepath_set = self._read_dirs(self.input_path)\n",
    "        # sample to test\n",
    "        # self.sample_json_path = self.tweets_filepath_set[\"2020-01\"][0]\n",
    "\n",
    "        self.input_df_path = [\n",
    "            os.path.join(self.input_path, x)\n",
    "            for x in os.listdir(self.input_path)\n",
    "            if x.endswith(\".csv\")\n",
    "        ]\n",
    "\n",
    "        self.model_path = os.path.join(\n",
    "            self.scratch_path, \"twitter-action/tfmodel/tweets_model2.h5\"\n",
    "        )\n",
    "        self.embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        self.model = keras.models.load_model(self.model_path)\n",
    "\n",
    "    def _read_dirs(self, input_path):\n",
    "        tweets_file_set = {}\n",
    "        for month_folder in os.listdir(input_path):\n",
    "            if month_folder.startswith(\"2020\") and not month_folder.endswith(\".zip\"):\n",
    "                tweets_file_set[month_folder] = []\n",
    "                month_folder_path = os.path.join(input_path, month_folder)\n",
    "                # print(month_folder_path)\n",
    "                for tweets_file in os.listdir(month_folder_path):\n",
    "                    if tweets_file.endswith(\"csv\"):\n",
    "                        tweets_file_path = os.path.join(month_folder_path, tweets_file)\n",
    "                        tweets_file_set[month_folder].append(tweets_file_path)\n",
    "\n",
    "        print(\"filepath:\", tweets_file_set.keys())\n",
    "        # print(\"all file count\", sum([len(tweets_file_set[x]) for x in tweets_file_set]))\n",
    "        return tweets_file_set\n",
    "\n",
    "    def _get_cleaned_text_list_sample(self, df):\n",
    "        \"\"\"deprecated function\"\"\"\n",
    "        embededText = self.embed(df[\"cleaned_text\"])\n",
    "        y_pred = model.predict(embededText, use_multiprocessing=True, batch_size=1024)\n",
    "\n",
    "    def _get_stressful_index(self, text_list):\n",
    "        return self.model.predict(\n",
    "            self.embed(text_list), use_multiprocessing=True, batch_size=1024\n",
    "        )\n",
    "\n",
    "    def _write_df_windows(self, df_sample, df_path):\n",
    "        output_path = df_path.replace(self.input_folder, self.output_folder)\n",
    "        print(\"writing\", output_path, df_sample.shape)\n",
    "        df_sample.to_csv(output_path, index=False)\n",
    "\n",
    "    def process_df(self, df_path):\n",
    "        df_sample = pd.read_csv(df_path, lineterminator=\"\\n\")\n",
    "        df_sample = df_sample[df_sample[\"cleaned_text\"].notna()]\n",
    "        # df_sample = df_sample[df_sample[\"CountyId\"].notna()]\n",
    "        print(df_sample.shape)\n",
    "\n",
    "        if df_sample.shape[0] != 0:\n",
    "\n",
    "            # predict text\n",
    "            y_pred = self._get_stressful_index(df_sample[\"cleaned_text\"].values)\n",
    "            df_sample[\"stress_rate\"] = y_pred\n",
    "\n",
    "        self._write_df_windows(df_sample, df_path)\n",
    "        del df_sample\n",
    "\n",
    "    def process_df_windows(self, df_path_list):\n",
    "        for df_path in df_path_list:\n",
    "            print(\"reading\", df_path)\n",
    "            output_path = df_path.replace(self.input_folder, self.output_folder)\n",
    "            if os.path.isfile(output_path):\n",
    "                continue\n",
    "            self.process_df(df_path)\n",
    "        print(\"all done.\")\n",
    "\n",
    "\n",
    "GS = GetStress()\n",
    "# GS.process_df_windows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-03-31.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-02-25.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-03-16.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-02-05.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-02-10.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-04-10.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-02-20.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-01-21.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-03-06.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-03-21.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-04-05.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-01-31.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-01-26.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-03-11.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-02-15.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-03-26.csv\n",
      "reading /scratch/user/diya.li/covid-map/twitter-dataset-processed-2/2020-03-01.csv\n",
      "(1047835, 15)\n",
      "writing /scratch/user/diya.li/covid-map/twitter-dataset-processed-stress/2020-03-01.csv (1047835, 16)\n",
      "all done.\n"
     ]
    }
   ],
   "source": [
    "df_path_list = GS.input_df_path\n",
    "GS.process_df_windows(df_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code below\n",
    "\n",
    "def load_df_windows(input_df_path):\n",
    "    for each_input_df_path in input_df_path:\n",
    "        print(\"reading\", each_input_df_path)\n",
    "        load_df_samples(each_input_df_path)\n",
    "    print(\"all sample done.\")\n",
    "\n",
    "\n",
    "def load_df_samples(sample_df_path):\n",
    "    df_sample = pd.read_csv(sample_df_path)\n",
    "    df_sample = df_sample[df_sample[\"cleaned_text\"].notna()]\n",
    "    df_sample = df_sample[df_sample[\"place_type\"].notna()]\n",
    "    print(df_sample.shape)\n",
    "\n",
    "    # load model and get stressful text\n",
    "    model = keras.models.load_model(\"tweets_model2.h5\")\n",
    "    y_pred = get_stressful_index(df_sample[\"cleaned_text\"], model)\n",
    "    df_sample[\"stress_rate\"] = y_pred\n",
    "    print(df_sample.shape, \"done.\")\n",
    "\n",
    "    print(\"Start writing to\", sample_df_path)\n",
    "    write_df_windows(df_sample, sample_df_path)\n",
    "    print(\"all done\")\n",
    "\n",
    "\n",
    "def get_stressful_index(text_list, model):\n",
    "    y_pred = model.predict(embed(text_list), use_multiprocessing=True, batch_size=1024)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def write_df_windows(df_sample, sample_df_path):\n",
    "    df_sample.to_csv(output_path + sample_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_df_windows(input_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code below is trying to fix those conflict between 1 and 2\n",
    "# import tensorflow.compat.v1 as tf\n",
    "\n",
    "# tf.disable_v2_behavior()\n",
    "# print(tf.version.VERSION)\n",
    "\n",
    "# # check issues below for above code\n",
    "# # https://stackoverflow.com/questions/57968999/runtimeerror-attempting-to-capture-an-eagertensor-without-building-a-function\n",
    "# cfg = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "# cfg.gpu_options.allow_growth = True\n",
    "# # version compat issues 1\n",
    "# def get_text_embedding_compat_v1(text_list):\n",
    "#     # tf.compat.v1.enable_eager_execution()\n",
    "#     with tf.compat.v1.Session(config=cfg) as session:\n",
    "#         session.run(\n",
    "#             [\n",
    "#                 tf.compat.v1.global_variables_initializer(),\n",
    "#                 tf.compat.v1.tables_initializer(),\n",
    "#             ]\n",
    "#         )\n",
    "#         return session.run(embed(text_list))\n",
    "\n",
    "\n",
    "# # # version compat issues 2\n",
    "# def get_text_embedding_v1(text_list):\n",
    "#     with tf.Session(config=cfg) as session:\n",
    "#         session.run(\n",
    "#             [tf.global_variables_initializer(), tf.tables_initializer(),]\n",
    "#         )\n",
    "#         return session.run(embed(text_list))\n",
    "\n",
    "\n",
    "# check is https://www.tensorflow.org/guide/effective_tf2\n",
    "# to explain more about the migration from tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChunks(textList):\n",
    "    chunksSize = 100000\n",
    "    chkTimes = len(textList) // chunkSize\n",
    "    chkLast = len(textList) - chkTimes * chunksSize\n",
    "    cur = 0\n",
    "    for chk in range(chkTimes):\n",
    "        firstSlice = cur\n",
    "        lastSlice = chunksSize * (cur + 1)\n",
    "        embedText = textList[firstSlice:lastSlice]\n",
    "        embededText = embed()\n",
    "\n",
    "\n",
    "embededText = embed(df_sample[\"cleaned_text\"])\n",
    "# will crash runs on 16GB 4Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embededText = get_text_embedding_compat_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
